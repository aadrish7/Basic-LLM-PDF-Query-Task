{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Code to mount the drive, so that it could be used to access the files and directories from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEuyElqufqEK",
        "outputId": "3379c5de-d3a6-4d60-af72-cedfab64bd6d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "i-C5dtVoYYzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the first method, I utilized Hugging Face's pipeline for question answering directly. This pipeline provides a straightforward abstraction for performing question answering tasks without the need for explicit model loading, tokenization, or inference coding.**"
      ],
      "metadata": {
        "id": "amuKruxYWIpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "CrLHBBVkboSo"
      },
      "outputs": [],
      "source": [
        "#Provided code imports libraries related to task one, then initializes a question-answering pipeline using a pre-trained model.\n",
        "from transformers import pipeline\n",
        "from pdfquery import PDFQuery\n",
        "oracle = pipeline(model=\"deepset/roberta-base-squad2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code defines a function to retrieve a list of files within a specified path**"
      ],
      "metadata": {
        "id": "PJKz4fv6XGkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get a list of files in a folder\n",
        "def get_files_in_folder(folder_path):\n",
        "    list_of_files = []\n",
        "    file_names = os.listdir(folder_path)\n",
        "    for file_name in file_names:\n",
        "        list_of_files.append(file_name)\n",
        "    return list_of_files\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/PDF Dataset\"\n",
        "file_list = get_files_in_folder(folder_path)  # Get list of files in the folder\n"
      ],
      "metadata": {
        "id": "CBkcSf-m5wFB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The purpose of the code is to extract text from multiple PDF files stored in a specific directory, combine the extracted text into a single string, and store it in the variable combined_text.**"
      ],
      "metadata": {
        "id": "LO3qstq2XePv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(file_path):\n",
        "    pdf = PDFQuery(file_path)\n",
        "    pdf.load()\n",
        "    text_elements = pdf.pq('LTTextLineHorizontal')\n",
        "    text = [t.text for t in text_elements]\n",
        "    return ' '.join(text)  # Combine extracted text into a single string\n",
        "\n",
        "# Function to combine text from multiple PDF files\n",
        "def combine_text_from_files(file_names):\n",
        "    combined_text = \"\"\n",
        "    for file_name in file_names:\n",
        "        if file_name.endswith(\".pdf\"):  # Check if file is a PDF\n",
        "            file_path = f\"/content/drive/My Drive/PDF Dataset/{file_name}\"  # Construct full file path\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "            combined_text += text + \"\\n\"  # Append extracted text to combined text separated with a line between documents\n",
        "    return combined_text\n",
        "\n",
        "combined_text = combine_text_from_files(file_list)  # Combine text from all PDF files\n"
      ],
      "metadata": {
        "id": "ywUUTmBo6co9",
        "collapsed": true
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query 1**"
      ],
      "metadata": {
        "id": "t5-7b5p7X5z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = oracle(question=\"Whats the CGPA of Aadrish?\", context=combined_text)#I also added my resume in the documents, to better understand the confirmation of information\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdaaCMJD3iMb",
        "outputId": "b0cc852d-bc4a-43c6-dcae-5dc5eb81d59b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.03149282559752464, 'start': 360, 'end': 367, 'answer': '3.3/4.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query 2**"
      ],
      "metadata": {
        "id": "WRie6f_4X9cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = oracle(question=\"Which technology is used for auditable access control for business processes?\", context=combined_text)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-ZMWlPg8-EU",
        "outputId": "22fe8a57-f7ea-4cf8-9e73-897b5fc748f0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.7222951650619507, 'start': 2220, 'end': 2231, 'answer': 'blockchains'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Part 2"
      ],
      "metadata": {
        "id": "T8cpVWTBYLt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code imports modules and sets up configurations for using language embeddings, vector stores, question-answering chains, document loaders, and text splitters from the langchain and langchain_community libraries, and initializes Hugging Face embeddings for natural language processing tasks.**"
      ],
      "metadata": {
        "id": "jqzuGsjoDDsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules for language embeddings and vector stores\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_core.documents import Document\n",
        "# Importing document loaders and text splitters\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Importing Hugging Face embeddings and setting up environment variable\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"  # Setting up Hugging Face API token\n",
        "embeddings = HuggingFaceEmbeddings()  # Initializing Hugging Face embeddings\n"
      ],
      "metadata": {
        "id": "nFvgbPDrszoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336b5a5b-5f3b-433b-c2df-aa8add329314"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code creates a list of documents extracted from PDF files in a specified directory, processes each PDF file to split its content into individual pages, and then creates Document objects from the extracted content along with their metadata. Finally, it uses the FAISS library to index the documents based on their embeddings for efficient search and retrieval.**"
      ],
      "metadata": {
        "id": "v6CTPoMvZEBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_documents = []\n",
        "\n",
        "pdf_directory = \"/content/drive/MyDrive/PDF Dataset\"\n",
        "for pdf_file in os.listdir(pdf_directory):\n",
        "    if pdf_file.endswith(\".pdf\"):  # Check if file is a PDF\n",
        "        print(pdf_file)  # Print the PDF file name\n",
        "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        pages = loader.load_and_split()\n",
        "        # Initialize text splitter with specified parameters\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1024,\n",
        "            chunk_overlap=64,\n",
        "            separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', '']\n",
        "        )\n",
        "        docs = text_splitter.split_documents(pages)\n",
        "        for i, page_content in enumerate(docs):  # Iterate over each document\n",
        "            page_content_str = str(page_content)\n",
        "            list_of_documents.append(Document(page_content=page_content_str, metadata=dict(page=i+1)))\n",
        "\n",
        "# Create FAISS index from the list of documents using embeddings\n",
        "db = FAISS.from_documents(list_of_documents, embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCl5tsanEeOG",
        "outputId": "e18d4e95-9445-495f-d024-3686d4c0a1ec"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume_Aadrish (One Page).pdf\n",
            "mypdf.pdf\n",
            "weblight.pdf\n",
            "slides-biasws-a-framework-for-improving-web-affordability-and-inclusiveness-00.pdf\n",
            "Coal_not_diamonds.pdf\n",
            "rethinking-web-affordability-inclusion.pdf\n",
            "2110.14205v1.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up the Model**"
      ],
      "metadata": {
        "id": "cMA-sqjZZLO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":1, \"max_length\":1000000})  # Initialize Hugging Face model\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")  # Load question-answering chain"
      ],
      "metadata": {
        "id": "cNUGZ5ANZIjN"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query 1**"
      ],
      "metadata": {
        "id": "45id4UomZR_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is CGPA of Aadrish?\"#I also added my resume in the documents, to better understand the confirmation of information\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fZpHJi5qJNa1",
        "outputId": "3a89c1e6-e706-463c-84ca-51bfd359a6bf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3/4.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query 2**"
      ],
      "metadata": {
        "id": "-fDaa60Fhmu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what was the main vulnerability ammar tahir found in google web light service?\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TNitWYX8d2U1",
        "outputId": "24a3f30e-1855-44cf-b77f-c6bb6b1305ce"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Web Light can potentially build usersâ€™ browsing profiles and read website content being viewed by users'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ]
}